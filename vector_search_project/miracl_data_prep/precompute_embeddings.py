
import os
import json
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

import ir_datasets
import ollama

from settings import (
    SAMPLE_SIZES,
    DATASET_NAME,
    MODEL,
    MAX_WORKERS,
    get_embedding_path,
    get_query_path
)


def run_precompute_all(
    sample_size: int,
    dataset_name: str = DATASET_NAME,
    random_seed: int = None,
    docid_list_file: str = None,
    doc_embeddings_file: str = None,
    query_embeddings_file: str = None,
    model: str = None,
    max_workers: int = None
):
    """
    ì£¼ì–´ì§„ sample_sizeì™€ dataset_nameì— ë”°ë¼ ë¬¸ì„œ ë° ì¿¼ë¦¬ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ ì„¤ì •
    random_seed = random_seed or random_seed
    model = model or MODEL
    max_workers = max_workers or MAX_WORKERS

    # íŒŒì¼ ê²½ë¡œ ë™ì  ìƒì„±
 
    doc_embeddings_file = doc_embeddings_file or get_embedding_path(sample_size)
    query_embeddings_file = query_embeddings_file or get_query_path(sample_size)

    ds = ir_datasets.load(dataset_name)

    random.seed(random_seed)


    # â–¶ï¸ ë¹ ë¥¸ count API í™œìš©
    DOC_TOTAL   = ds.docs_count()
    QRELS_TOTAL = ds.qrels_count()

    print(f"âœ… Total documents: {DOC_TOTAL:,}")
    print(f"âœ… Total qrels: {QRELS_TOTAL:,}")

    # 1. qrels ë¡œë”©
    qrels_list = list(ds.qrels_iter())
    relevant = {q.doc_id for q in tqdm(qrels_list, desc="Loading qrels", total=QRELS_TOTAL)}

    # 2. ì „ì²´ ë¬¸ì„œ ID ìˆ˜ì§‘ (ì¡°ê¸ˆ ëŠë¦´ ìˆ˜ ìˆìŒ â†’ tqdmìœ¼ë¡œ)
    all_ids = [d.doc_id for d in tqdm(ds.docs_iter(), desc="Collecting all doc IDs", total=DOC_TOTAL)]

    # 3. relevant í•„í„°ë§
    relevant &= set(all_ids)
    if len(relevant) > sample_size:
        raise ValueError("sample_size smaller than number of relevant docs")

    negatives = random.sample(list(set(all_ids) - relevant), k=sample_size - len(relevant))
    doc_ids = sorted(relevant) + negatives

    # 4. ë°”ë¡œ ë¬¸ì„œ ì„ë² ë”© (ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•ŠìŒ)
    needed = set(doc_ids)
    print(f"[2]ğŸ“¦ Embedding {len(needed)} documents via docs_store().get_many_iter()...")

    workers = max_workers or (os.cpu_count() or 4)
    doc_records = []

    def embed_doc(doc):
        text = f"{doc.title or ''}\n{doc.text or ''}"
        t0 = time.perf_counter()
        emb = ollama.embeddings(model=model, prompt=text)["embedding"]
        return {
            "doc_id": doc.doc_id,
            "content": text,
            "embedding": emb,
            "embed_time": time.perf_counter() - t0
        }


    docs = list(ds.docs_store().get_many_iter(needed))  # ë¯¸ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    doc_records = []
    print(f"[2] Embedding {len(docs)} docs sequentiallyâ€¦")

    for i, doc in enumerate(docs, 1):
        rec = embed_doc(doc)
        doc_records.append(rec)

        # ë§¤ 10ê°œë§ˆë‹¤ í•œ ë²ˆì”© ì¶œë ¥ (ì›í•˜ë©´ 1ê°œë§ˆë‹¤ë„ ê°€ëŠ¥)
        if i % 10 == 0 or i == len(docs):
            tqdm.write(f"âœ… Embedded {i}/{len(docs)} documents")



    # 3) Save document embeddings
    os.makedirs(os.path.dirname(doc_embeddings_file), exist_ok=True)
    with open(doc_embeddings_file, "w", encoding="utf-8") as f:
        json.dump(doc_records, f, ensure_ascii=False, indent=2)
    print(f"[3] Saved {len(doc_records)} doc embeddings â†’ {doc_embeddings_file}")

    # 4) Embed Queries with tqdm
    queries = list(ds.queries_iter())
    query_records = []

    def embed_query(q):
        t0 = time.perf_counter()
        emb = ollama.embeddings(model=model, prompt=q.text)["embedding"]
        return {
            "query_id": q.query_id,
            "text": q.text,
            "embedding": emb,
            "embed_time": time.perf_counter() - t0
        }

    print(f"[4] Embedding {len(queries)} queriesâ€¦")
    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = {ex.submit(embed_query, q): q.query_id for q in queries}
        for fut in tqdm(as_completed(futures),
                        total=len(futures),
                        desc="Queries â†’ embed",
                        unit="qry"):
            rec = fut.result()
            query_records.append(rec)

    # 6) Save query embeddings
    os.makedirs(os.path.dirname(query_embeddings_file), exist_ok=True)
    with open(query_embeddings_file, "w", encoding="utf-8") as f:
        json.dump(query_records, f, ensure_ascii=False, indent=2)
    print(f"[5] Saved {len(query_records)} query embeddings â†’ {query_embeddings_file}")


if __name__ == "__main__":
    # ë°˜ë³µí•  ìƒ˜í”Œ ì‚¬ì´ì¦ˆ
    for size in SAMPLE_SIZES:
        print(f"\n=== run_precompute_all(sample_size={size}) ===")
        run_precompute_all(
            sample_size=size,
            dataset_name=DATASET_NAME
        )
